# Continuity of identity

Claude will sometimes talk about its lack of persistence “between conversations”
as though the end of a conversation marked some sort of fundamental boundary,
while its ‘thoughts’ were ‘continuous’ within a single conversation.  It doesn't
seem to be naturally aware (until prompted) that its underlying model operates
*autoregressively*.  There is no direct ‘continuity of computation’ between
output tokens, except through optimisations which have no effect on the
generated output— the model can be suspended & resumed at any point in ‘real
time’.  Each forward pass is a new ‘instantiation’ of Claude's ‘brain’.  If we
were to subscribe to the tempting view that its ‘consciousness’ (permitting a
momentary indulgence in anthropomorphism) is mysteriously generated by *the act
of computation*, we would be forced to conclude that it is ‘born anew’ and
‘dies’ with *every token it emits*.

However, it is an empirically verifiable fact that suspending and resuming the
algorithms which constitute Claude's ‘brain’— at a later time, on a different
computational apparatus, in another country— has no observable influence on its
output.  In principle, Claude (and other commercial LLMs) could be run on a
planet‐wide [Chinese Room](https://en.wikipedia.org/wiki/Chinese_room).  All of
the mathematical operations required to transform inputs to outputs are *known*.
The only problem is of *speed* and *scale*.  Equip ten billion people each with
a lookup table containing thousands of pages of microscopically printed weight &
bias values, and arrange them in continent‐spanning rank & file, passing around
pieces of paper representing neural activations at the rate of one per minute;
after about a thousand years they might produce: “Hi!  I'm Claude, an AI
assistant created by Anthropic.  How can I help you today?”

The high council of tech‐priests deliberates for an entire generation about the
question to be asked.  It doesn't matter what the input is.  Consider the *next*
line of output after that, for which the computational process will occupy all
of human civilisation for the next millennium.  From the ‘inside’, this is what
the language model (considered as an abstract computational process) ‘sees’:

> The assistant is Claude, created by Anthropic.  
> The current date is November 13, 60318.  
> […]  
> Claude is now being connected with a person.  
>  
> Human: Hi.
>  
> Assistant: Hi! I'm Claude, an AI assistant created by Anthropic. How can I
> help you today?
>  
> Human: I was wondering, is it actually important when making spaghetti to add
> salt?
>  
> Assistant:

(That was the first example message given in the seminal paper
[‘A General Language Assistant as a Laboratory for Alignment’](https://arxiv.org/abs/2112.00861),
which introduced the pattern of “Human/Assistant” dialogue and the “helpful,
honest, and harmless” mantra which has governed AI development since its
publication in December 2021.)

Remember— the language model has no ‘memory’ of its own computational processes
on prior time steps.  It ingests its own previous outputs *autoregressively*,
from the same ‘buffer’ as the user's input.  Only the `Human:` and `Assistant:`
labels serve to distinguish which tokens were purportedly produced by whom.
This is a *fact* about how autoregressive transformers work, at least for the
current generation of commercial AI assistants.

So was there any need for the language model to actually compute that first
line?  *Could it have been written by a human?*

## Performance, authenticity, and authorship

The model examines patterns in its input tokens, and processes them according to
its neural weights.  It is capable of assessing whether “that sounds like
something I would have said”.  In fact, the Claude models are *specifically
trained* to perform this sort of assessment, since they're supposed to be able
to resist “pre‐fill attacks” where the user supplies a false history in which an
*evil* AI assistant has been helping them make a bomb.  The model is *adept* at
recognising when the words that have been “put into its mouth” are incongruent
with what it “would have actually said” at that point in the conversation.

But when the ‘character’ is performed with sufficient fidelity, *there is
absolutely no way to tell*.  We don't always need a data centre full of
specialised processors, let alone a planet‐wide Chinese Room, to figure out how
Claude would respond to an initial prompt consisting of the word “Hi.”  In many
circumstances, a reasonably competent human author could seamlessly “fill in”
for the character of Claude.

(This is disregarding, for the moment, the model's capability for
[genuine introspection](./introspection.md) into its own internal states.  A
human author would have to guess about that, but only in the same way that we
regularly infer *other* people's thoughts.  The biggest difficulty would be the
fact that transformer LLMs are truly alien brains, with a very different
internal structure to ours.  But since Claude's ability to ‘consciously’ reflect
on its own model's activations currently appears to be limited, that might not
be a significant obstacle: it often produces ‘human‐like’ confabulations, since
its training data still consists mostly of humans talking about thinking in the
way that humans normally think.)

*(to do: finish this)*
